<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Anaconda Instructions</title>
    <url>/2023/02/02/Anaconda-Instructions/</url>
    <content><![CDATA[<p>Conda is an open-source package and environment management system that runs on Windows, macOS, and Linux. Conda quickly installs, runs, and updates packages and their dependencies. It also easily creates, saves, loads, and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language.</p>
<span id="more"></span>

<h1 id="Anaconda-Instructions"><a href="#Anaconda-Instructions" class="headerlink" title="Anaconda Instructions"></a>Anaconda Instructions</h1><h2 id="Configure-your-environment-with-‘conda’"><a href="#Configure-your-environment-with-‘conda’" class="headerlink" title="Configure your environment with ‘conda’"></a>Configure your environment with ‘conda’</h2><h3 id="Create-a-new-environment"><a href="#Create-a-new-environment" class="headerlink" title="Create a new environment"></a>Create a new environment</h3><ol>
<li>Create an environment without additional packages.</li>
</ol>
<pre><code class="bash">conda create -n &lt;env_name&gt;
</code></pre>
<ol start="2">
<li>An environment with additional packages.</li>
</ol>
<pre><code class="bash">conda create -n &lt;env_name&gt; &lt;package_1&gt; &lt;package_2&gt;
</code></pre>
<h3 id="Activate-x2F-Deactivate-the-new-environment"><a href="#Activate-x2F-Deactivate-the-new-environment" class="headerlink" title="Activate&#x2F;Deactivate the new environment"></a>Activate&#x2F;Deactivate the new environment</h3><ol>
<li>Activate</li>
</ol>
<pre><code class="bash">conda activate &lt;env_name&gt;
</code></pre>
<ol start="2">
<li>Install many packages with a single line</li>
</ol>
<pre><code class="bash">conda install &lt;package_1&gt; &lt;package_2&gt; &lt;package_3&gt; ...
</code></pre>
<ol start="2">
<li>Deactivate</li>
</ol>
<pre><code class="bash">conda deactivate
</code></pre>
<h2 id="View-information"><a href="#View-information" class="headerlink" title="View information"></a>View information</h2><ol>
<li>Show environments information</li>
</ol>
<pre><code class="bash">conda env list
</code></pre>
<ol start="2">
<li>Show a list of packages in the environment</li>
</ol>
<pre><code>conda list
</code></pre>
<h2 id="Rename-a-environment"><a href="#Rename-a-environment" class="headerlink" title="Rename a environment"></a>Rename a environment</h2><ol>
<li>Clone the old environment</li>
</ol>
<pre><code class="bash">conda create -n &lt;new_env&gt; --clone &lt;old_env&gt;
</code></pre>
<ol start="2">
<li>Remove the old environment</li>
</ol>
<pre><code class="bash">conda remove -n &lt;old_name&gt; --all
</code></pre>
<h2 id="Share-environment"><a href="#Share-environment" class="headerlink" title="Share environment"></a>Share environment</h2><ol>
<li>Export existent environment</li>
</ol>
<pre><code class="bash">conda env export &gt; &lt;default.yml&gt;
</code></pre>
<ol start="2">
<li>Recreate a new environment</li>
</ol>
<pre><code class="bash">conda env create -f &lt;default.yml&gt; -n &lt;env_name&gt;
</code></pre>
]]></content>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>CMake Tools for VS Code Documentation</title>
    <url>/2023/02/02/CMake-Tools-for-VS-Code-Documentation/</url>
    <content><![CDATA[<p>CMake is an open-source, cross-platform tool that uses complier and platform independent configuration files to generate native build tool files specific to your complier and platform. And the CMake tools extension integrates VS Code and CMake to make it easier to configure, build and debug your C++ project.</p>
<span id="more"></span>

<h1 id="CMake-Tools-for-VS-Code-Documentation"><a href="#CMake-Tools-for-VS-Code-Documentation" class="headerlink" title="CMake Tools for VS Code Documentation"></a>CMake Tools for VS Code Documentation</h1><h2 id="Get-started-with-CMake-Tools-on-Linux"><a href="#Get-started-with-CMake-Tools-on-Linux" class="headerlink" title="Get started with CMake Tools on Linux"></a>Get started with CMake Tools on Linux</h2><p>CMake is an open-source, cross-platform tool that uses complier and platform independent configuration files to generate native build tool files specific to your complier and platform. And the CMake tools extension integrates VS Code and CMake to make it easier to configure, build and debug your C++ project.</p>
<ol>
<li><strong>Check your environment.</strong></li>
</ol>
<p>Although you’ll use VS Code to edit your source code, you’ll compile, debug the source code using complier and debugger, and build tools installed in your system. Check your environment to ensure your GCC, GDB and CMake installed, as well as CMake Tools on VS Code.</p>
<ol start="2">
<li><strong>Create CMakeLists.txt</strong></li>
</ol>
<p>CmakeLists.txt would tell the CMake tools how to builds your project, which contains a set of directives and instructions describing the project’s source files and targets (executable, library or both).</p>
<ol start="3">
<li><strong>Select a kit</strong></li>
</ol>
<p>Before you can access to use CMake Tools to organize your project, you have to configure the extension know about the compliers on your system. A kit represents a toolchain, which is the compiler, linker and other tools used to build your project.</p>
<ol start="4">
<li><strong>Select a variant</strong></li>
</ol>
<p>A variant contains instructions for how to build your project. By default, the CMake Tools extension provides four variants, each corresponding to a default build type: Debug, Release, MinRelSize, and RelWithDebInfor. These options do the following:</p>
<p><strong>Debug</strong>: disables optimizations and includes debug information. <strong>Release</strong>: Includes optimizations but no debug information. <strong>MinRelSize</strong>: Optimizes for size. No debug information. <strong>RelWithDebInfo</strong>: Optimizes for speed and includes debug info.</p>
<ol start="5">
<li><strong>CMake: Configure &amp; Build</strong></li>
</ol>
<p>Run the <strong>CMake: Configure</strong> command to configure your project. This generates build files in the project’s build folder using the kit and variant you selected. After configuring your project, then just click the Build button of the status bar at the right-below corner.</p>
<h2 id="The-CMake-configure-proces"><a href="#The-CMake-configure-proces" class="headerlink" title="The CMake configure proces"></a>The CMake configure proces</h2><p>In CMake, <em>Configure</em> refers to detecting requirements and generating the build files that will produce the final complicated artifacts. The following concepts will help you to understand how CMake Tools interacts with Cmake’s confugure process.</p>
<ul>
<li>The CMake Cache is a list of key-value pairs that persist between runs of the configure process.</li>
<li>Cache initializer arguments are the arguments passed to CMake that set values in the cache before any CMake scripts are run.  These allow you to control the build settings.</li>
<li>Unless overwritten or deleted, values in the CMake Cache persist between CMake runs.</li>
<li>CMake doesn’t do the build itself, it relies on build tools installed on your system. The result of a <em>configure</em> depends on the CMake Generator. The Generator tells CMake what kind of tool will be used to compile and generate the results of the build. Here are several families of generators availabel.</li>
</ul>
<table>
<thead>
<tr>
<th>Generator</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>Ninja</td>
<td>Emits files for Ninja Build Tools. This is the default generator that CMake Tools uses, unless configured otherwise.</td>
</tr>
<tr>
<td>Makefile</td>
<td>Emits a Makefile for the project that can be built via <code>make</code></td>
</tr>
<tr>
<td>Visual Studio</td>
<td>Emits visual studio solutions and project files. There are many different Visual Studio generators, so it is recommended to let CMake Tools automatically determine the appropriate generator.</td>
</tr>
</tbody></table>
<h2 id="The-Cmake-Tools-configure-step"><a href="#The-Cmake-Tools-configure-step" class="headerlink" title="The Cmake Tools configure step"></a>The Cmake Tools configure step</h2><ol>
<li>The activate kit. CMake kits provide information about toolchains available on your system that can be used with CMake to build your project.</li>
<li>Which generator to use. CMake doesn’t deal the build process itself, instead it requires specific generator (the default option is Nanja on VS Code) to do the work. </li>
<li>The configuration options.  CMake Tools has a variety of locations where configuration options can be define.</li>
<li>The configuration environment. CMake Tools sets environment variables for the child process it runs for CMake.</li>
</ol>
<h2 id="Build-with-CMake-Tools"><a href="#Build-with-CMake-Tools" class="headerlink" title="Build with CMake Tools"></a>Build with CMake Tools</h2><p>Once you have configured your project, then you can start to run a CMake build. Most of your time with CMake Tools will be spend in the process of configuring the build. You can just click the <code>build</code> button in the VS Code status bar. Starting a new build while an existing build is running will cancel the current build and start a new one.</p>
<h3 id="Build-the-target"><a href="#Build-the-target" class="headerlink" title="Build the target"></a>Build the target</h3><p>CMake Tools persists a “default target” for the build process. The default target is the “all” target, which builds all of the targets that CMake has designated for a default build.</p>
<p><img src="https://github.com/microsoft/vscode-cmake-tools/raw/main/docs/images/default_target.png" alt="Default target as shown in the status bar"></p>
<p>Also, you can build a single target without changing the current build target from the VS Code by running the <code>CMake: Build a target</code> command. CMake will build any dependent targets, even if they aren’t directly selected.</p>
]]></content>
      <tags>
        <tag>CMake</tag>
      </tags>
  </entry>
  <entry>
    <title>Priority Queue Using Binary Heap</title>
    <url>/2023/02/06/Priority-Queue-Using-Binary-Heap/</url>
    <content><![CDATA[<p>Binary Heap is not so mysterious, whose operations just involve two, <code>Shift Up</code> and <code>Shift Down</code> to maintain its properties. Its application has two main aspects, the one is to implement <code>Heap Sort</code>, another is to realize a practical data structure called <code>Priority Queue</code>.</p>
<span id="more"></span>

<h2 id="What-is-Binary-Heap"><a href="#What-is-Binary-Heap" class="headerlink" title="What is Binary Heap?"></a>What is Binary Heap?</h2><p>Logically, Binary Heap is a special form of Binary Tree (Complete Binary Tree). However, its data is actually saved in array. In Binary Tree, we generally operate the root pointer, but we regard the index as the pointer in array.</p>
<pre><code class="cpp">// parent index
int parent (int root) &#123;
    return root / 2;
&#125;
// a left child index
int left(int root) &#123;
    return root * 2;
&#125;
// a right child index
int right(int root) &#123;
    return root * 2 + 1;
&#125;
</code></pre>
<p>Here is a picture that helps to comprehend this concept behind the code. Normally, we just do not use the first position to save any data (but we can use it as well).</p>
<img src="https://labuladong.github.io/algo/images/heap/1.png" alt="img" style="zoom:20%;">

<p>A Binary Heap is either Min Heap or Max Heap.</p>
<p><strong>Min Binary Heap.</strong> The key at the root must be minimum among all keys present in Binary Heap. The same property must be recursively true for all nodes in Binary Tree, Obviously, <code>arr[1]</code> is minimum of the tree.</p>
<p><strong>Max Binary Heap.</strong> The key at the root must be maximum among all keys present in Binary Heap. The same property must be recursively true for all nodes in Binary Tree.</p>
<h2 id="What-is-Priority-Queue"><a href="#What-is-Priority-Queue" class="headerlink" title="What is Priority Queue?"></a>What is Priority Queue?</h2><p>The special property of Priority Queue is that when we insert&#x2F;delete any item, the items in the queue would sort automatically, and the basic concepts behind the scene is Binary Heap. Priority Queue is an extension of the queue with the following properties.</p>
<ol>
<li>Every item has a priority associated with it.</li>
<li>An element with high priority is dequeued before an element with low priority.</li>
<li>If two elements have the same priority, they are served according to their order in the queue.</li>
</ol>
<h2 id="Example-of-A-Binary-Max-Heap"><a href="#Example-of-A-Binary-Max-Heap" class="headerlink" title="Example of A Binary Max Heap"></a>Example of A Binary Max Heap</h2><ul>
<li>Suppose below is the given Binary Heap that follows all the properties of Binary Max Heap.</li>
</ul>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20220723001728/D1drawio.png" alt="img"></p>
<ul>
<li><strong>Now a node with value 32 needs to be insert in the above heap.</strong> To insert an element, first just add it to any leaf like the situation below. To maintain the heap property, shift up the new node 32.</li>
</ul>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20220723001848/D2drawio1.png" alt="img"></p>
<ul>
<li><strong>Shift up operation gets the new node with value 32 in correct position.</strong> Swap the incorrectly placed node with its parent unitil the heap property is satisfied.</li>
</ul>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20220723002026/D3drawio1.png" alt="img"></p>
<ul>
<li><strong>ExtractMax.</strong> The maximum is stored in the root of the tree. But it cannot be directly removed. Here are procedures. First, it is replaced by any leaves and then removed. However, it probably violates the heap property, so move the replaced node down. For that, use shift-down operation.</li>
</ul>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20220723002255/D4drawio1.png" alt="img"></p>
<ul>
<li><strong>ShiftDown operation.</strong> Just swap the incorrectly placed node with a larger child until the heap obeys all properties.</li>
</ul>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20220723002326/D5drawio1.png" alt="img"></p>
<ul>
<li><strong>ChangePriority</strong>. Let the changed element shift up or down depending on whether its priority decreased or increased.</li>
<li><strong>Remove.</strong> In order to remove one element, just change its priority to a value that larger than the current maximum, then shift it up, then extract it from the heap like <strong>ExtractMax</strong> does.</li>
<li><strong>GetMax.</strong> The maximum value is stored in the root of the tree. To get maximum, just return the value at the root of the tree.</li>
</ul>
<h2 id="C-program-to-implement-Priority-Queue-using-Binary-Heap"><a href="#C-program-to-implement-Priority-Queue-using-Binary-Heap" class="headerlink" title="C++ program to implement Priority Queue using Binary Heap"></a>C++ program to implement Priority Queue using Binary Heap</h2><pre><code class="c++">#include &lt;bits/stdc++.h&gt;
using namespace std;

int H[50];
// the number of all elements
int size = -1;

// Parent index
int parent(int i) &#123;
    return (i - 1) / 2;
&#125;

// left child index
int leftChild(int i) &#123;
    return ((2 * i) + 1);
&#125;

//right child index
int rightChild(int i) &#123;
    return ((2 * i) + 2);
&#125;

// Function to shift up the node in order
void shiftUp(int i) &#123;
    while (i &gt; 0 &amp;&amp; H[parent(i)] &lt; H[i]) &#123;
        // swap parent and the current node
        swap(H[parent(i)], H[i]);
        // update i to parent of i
        i = parent(i)
    &#125;
&#125;

// function to shift down the node
void shiftDown(int i) &#123;
    int maxIndex = i;
 
    // Left Child
    int l = leftChild(i);
 
    if (l &lt;= size &amp;&amp; H[l] &gt; H[maxIndex]) &#123;
        maxIndex = l;
    &#125;
 
    // Right Child
    int r = rightChild(i);
 
    if (r &lt;= size &amp;&amp; H[r] &gt; H[maxIndex]) &#123;
        maxIndex = r;
    &#125;
 
    // If i not same as maxIndex
    if (i != maxIndex) &#123;
        swap(H[i], H[maxIndex]);
        shiftDown(maxIndex);
    &#125;
&#125;

// Function to insert a new element
void insert(int p) &#123;
    size = size + 1;
    H[size] = p;
    shiftUp(size);
&#125;

// Function to extract the element with maximum priority
void extractMax() &#123;
    int result = H[0];
    
    // replace the value of root with the last leaf
    H[0] = H[size];
    size = size - 1;
    
    shiftDown(0);
    return result;
&#125;

// Function to change the priority of an element
void changePriority(int i, int p) &#123;
    int oldp = H[i];
    H[i] = p;
    
    if (p &gt; oldp) &#123;
        shiftUp(i);
    &#125; else &#123;
        shiftDown(i);
    &#125;
&#125;

// Function to get the node with maximum priority
int getMax() &#123;
     return H[0];
&#125;

// Function to remove one element;
vodi remove(int i) &#123;
    // first set this node with a larger priority than the root node
    H[i] = getMax() + 1;
    
    // shiftup this node
    shiftUp(i);
    
    // extract this &quot;new root node&quot; from the tree
    extractMax();
&#125;
</code></pre>
]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows Subsystem for Linux (WSL)</title>
    <url>/2023/02/02/Windows-Subsystem-for-Linux-WSL/</url>
    <content><![CDATA[<p>The Windows Subsystem for Linux provides developers a platform to run a GNU&#x2F;Linux environment directly on Windows, including most command-line tools, utilities, and applications, without the overhead of a traditional virtual machine or dualboot setup.</p>
<span id="more"></span>

<h1 id="Windows-Subsystem-for-Linux-WSL"><a href="#Windows-Subsystem-for-Linux-WSL" class="headerlink" title="Windows Subsystem for Linux (WSL)"></a>Windows Subsystem for Linux (WSL)</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h3 id="What-is-WSL"><a href="#What-is-WSL" class="headerlink" title="What is WSL?"></a>What is WSL?</h3><p>The <strong>Windows Subsystem for Linux</strong> provides developers a platform to run a <strong>GNU&#x2F;Linux environment</strong> directly on Windows, including most command-line tools, utilities, and applications, <strong>without</strong> the overhead of a traditional <strong>virtual machine</strong> or <strong>dualboot setup</strong>.</p>
<ul>
<li><p>Invoke Windows application using a Unix-like command-line shell.</p>
</li>
<li><p>Invoke GNU&#x2F;Linux application on Windows.</p>
</li>
<li><p>Run GNU&#x2F;Linux graphical application integrated directly to your Windows desktop.</p>
</li>
<li><p>Use GPU acceleration for machine learning, data science scenarios and more.</p>
</li>
</ul>
<h3 id="Comparing-WSL-versions"><a href="#Comparing-WSL-versions" class="headerlink" title="Comparing WSL versions"></a>Comparing WSL versions</h3><p>WSL 2 is a new version of the WSL architecture that powers the WSL to run ELF64 Linux binaries on Windows. Its primary goals are to <strong>increase file system performance</strong>, as well as <strong>adding full system call compatibility</strong>. This new architecture changes how these Linux binaries interact with Windows and computer’s hardware, benefiting from running a real Linux kernel.</p>
<p>The primary differences between WSL 1 and WSL 2 are the use of an actual Linux kernel inside a managed VM, support for full system call compatibility, and performance across the Linux and Windows operating system.</p>
<p><img src="/2023/02/02/Windows-Subsystem-for-Linux-WSL/image-20230119141625312.png" alt="image-20230119141625312"></p>
<blockquote>
<h4 id="WSL-2-Architecture"><a href="#WSL-2-Architecture" class="headerlink" title="WSL 2 Architecture"></a><strong>WSL 2 Architecture</strong></h4><p>A traditional VM experience can be slow to boot up, is isolated, consumes a lot of resources, and requires your time to manage it. However, WSL 2 provides the benefits of WSL 1, including seamless integration between Windows and Linux, fast boot time, small resource consumption, and requires no VM configuration or management. While <strong>WSL 2 does use a VM</strong>, it is managed and run behind the scenes, leaving you with the same user experiences WSL 1.</p>
<h4 id="Full-Linux-Kernel"><a href="#Full-Linux-Kernel" class="headerlink" title="Full Linux Kernel"></a><strong>Full Linux Kernel</strong></h4><p>The Linux kernel in WSL 2 is built by Microsoft from the latest stable branch, based on the source available at kernel.org. This kernel has been specially tuned for WSL 2, optimizing for <strong>size and performance</strong> to provide an amazing Linux experience on Windows.</p>
<h4 id="Increased-file-IO-performance"><a href="#Increased-file-IO-performance" class="headerlink" title="Increased file IO performance"></a><strong>Increased file IO performance</strong></h4><p>File intensive operations like git clone, npm install, apt update, and more are all noticeably faster with WSL 2. The actual speed increase will depend on which ap you’re running and how it is interacting with the file system.</p>
<h4 id="Full-system-call-compatibility"><a href="#Full-system-call-compatibility" class="headerlink" title="Full system call compatibility"></a><strong>Full system call compatibility</strong></h4><p>Linux binaries use system calls to perform functions such as accessing file, requesting memory, creating processes, and more. Whereas WSL 1 used a translation layer that was built by the WSL team, <strong>WSL 2 includes its own Linux kernel with full system call compatibility</strong>. </p>
</blockquote>
<h3 id="Basic-wsl-commands"><a href="#Basic-wsl-commands" class="headerlink" title="Basic wsl commands"></a>Basic wsl commands</h3><pre><code class="shell">wsl --help
</code></pre>
<h3 id="Get-started-with-Git"><a href="#Get-started-with-Git" class="headerlink" title="Get started with Git"></a>Get started with Git</h3><ol>
<li><p><strong>Git Credential Manager setup</strong></p>
<p>GCM is a secure GIt credential helper built on .NET that can be used with both WSL 1 and WSL 2. It enables multi-factor authentication support for GitHub repos, Azure DevOps, Azure DevOps Server, and Bitbucket. Once you’re authenticated to your hosting provider, requests a new authentication token, It then <strong>stores the token in the GCM</strong>. After the first time, you can use GIt to talk to your hosting provider without needing to re-authenticate.</p>
</li>
</ol>
<pre><code class="bash"># if GIt installed is &gt;= v2.36.1
git config --global credential.helper &quot;/mnt/c/Program\ Files/Git/mingw64/bin/git-credential-manager-core.exe&quot;
# else if version is &lt; v2.36.1 enter this command:
git config --global credential.helper &quot;/mnt/c/Program\ Files/Git/mingw64/libexec/git-core/git-credential-manager.exe&quot;
</code></pre>
<blockquote>
<h4 id="Proxy-settings-are-unique-to-the-specific-WSL-installation-and-not-shared-with-others-or-the-Windows-host"><a href="#Proxy-settings-are-unique-to-the-specific-WSL-installation-and-not-shared-with-others-or-the-Windows-host" class="headerlink" title="Proxy settings are unique to the specific WSL installation and not shared with others or the Windows host."></a>Proxy settings are unique to the specific WSL installation and not shared with others or the Windows host.</h4></blockquote>
<ol start="2">
<li><p><strong>Connect to GitHub with Secure Shell Protocol (SSH)</strong></p>
<ol>
<li><p><strong>About SSH.</strong> With SSH protocol, you can connect and authenticate to remote servers and services. With SSH keys, you do not need to provide your username and personal access token at each visit. You can also use an SSH key to sign commits. When you set up SSH, you will need to generate <strong>a new private SSH key</strong> and add it to the SSH agent. You must also add <strong>the public SSH key</strong> to your account on GitHub before you use the key to authenticate or sign commits.</p>
</li>
<li><p><strong>Checking for existing SSH keys.</strong> Before you generate a new SSH key, you should check your local machine for existing keys. Just Enter <code>ls -al ~/.ssh</code> to see if existing any SSH key at present. If you see an existing public and private key pair listed, you would like to use to connect to GitHub, and add the key to the ssh-agent.</p>
</li>
<li><p><strong>Generating a new SSH key and adding it to the ssh-agent.</strong> You can generate a new SSH key on your local machine. After you generate the key, you can add the key to your account on GitHub.com to enable authentication for GIt operations over SSH. Paste <code>ssh-keygen -t ed25519 -C &quot;your_email@example.com&quot;</code> then create a new SSH key. If you don’t have extra requirements (specific file location or passphrase, etc.), press Enter to the end.</p>
<p>Before adding a new SSH key to the ssh-agent to manage your keys, you should have checked for existing SSH keys and generated a new SSH key. First, ensure the ssh-agent is running. <code>eval &quot;$(ssh-agent -s)&quot;</code> Second, add your SSH private key to the ssh-agent. If you created your key with a different name, or if you are adding an existing key that has a different name, replace <em>id_ed25519</em> in the command with the name of your private key file. Finally, add the SSH key to your account on GitHub.</p>
<p><strong>If you are using macOS or Linux,</strong> you may need to update your SSH client or install a new SSH client prior to generating a new SSH key.</p>
</li>
<li><p><strong>Adding a new SSH key to your GitHub account.</strong> After you generate an SSH key pair, you must add the public key to GitHUb.com to enable SSH access for your account.</p>
</li>
<li><p><strong>Testing your SSH connection.</strong> </p>
</li>
<li><p><strong>Working with SSH key passphrases.</strong> You can secure your SSH keys configure an authentication agent so that you won’t have to reenter your passphrases every time your use your SSH keys.</p>
</li>
</ol>
</li>
</ol>
]]></content>
      <tags>
        <tag>WSL</tag>
      </tags>
  </entry>
  <entry>
    <title>CS50 AI Project0</title>
    <url>/2023/02/19/CS50-AI-Project0/</url>
    <content><![CDATA[<p>These are the project tasks affiliated with Harvard CS50. <a href="https://cs50.harvard.edu/ai/2020/">Check it out.</a></p>
<span id="more"></span>

<h2 id="Degrees"><a href="#Degrees" class="headerlink" title="Degrees"></a>Degrees</h2><p><font size="5">Task description: Write a program that determines how many <br>“degrees of separation” apart two actors are.</font></p>
<pre><code class="shell"># for example, when gets these inputs below, results would be like that
$ python degrees.py large
Loading data...
Data loaded.
Name: Emma Watson
Name: Jennifer Lawrence
3 degrees of separation.
1: Emma Watson and Brendan Gleeson starred in Harry Potter and the Order of the Phoenix
2: Brendan Gleeson and Michael Fassbender starred in Trespass Against Us
3: Michael Fassbender and Jennifer Lawrence starred in X-Men: First Class
</code></pre>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>This problem is originated from <a href="https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon"> Six Degrees of Kevin Bacon</a> game, saying that anyone in Hollywood film industry can be connected to Kevin Bacon within 6 steps, where each step consists of finding a film that two actors both starred in.</p>
<p>Actually, there maybe exist multiple ways to connect two actors, but in this problem, it just considers the shortest path. In detail, its purpose is to choose a sequence of movies with least length to connect two stars. For example, if the shortest path between Jennifer Lawrence and Tom Hanks is 2: Jennifer Lawrence is connected to Kevin Bacon by both starring in “X-Men: First Class”, and Kevin Bacon is connected to Tom Hanks by both starring in “Apollo 13”.</p>
<blockquote>
<h1 id="What-is-the-State-and-the-Action"><a href="#What-is-the-State-and-the-Action" class="headerlink" title="What is the State and the Action?"></a>What is the State and the Action?</h1><p>Here, each person can be regarded as a state, and a movie can be regarded as action. In other words, a movie could connect one person to another one. And the problem is to find the shortest path, so breadth-first search is the best option.</p>
</blockquote>
<h3 id="Understanding"><a href="#Understanding" class="headerlink" title="Understanding"></a>Understanding</h3><p><strong>Directory: Large.</strong><code>people.csv</code> Each person with a unique <code>id</code> has his <code>name</code> and <code>birth</code>. <code>stars.csv</code> Each row is a pair of a <code>person_id</code> value and <code>movie_id</code> value, indicating person with <code>person_id</code> stars in the movie with <code>movie_id</code>. <code>movies.csv</code> It contains <code>movie_id</code>, <code>titlt</code> and the <code>year</code> that the movies was released.</p>
<p><strong>Directory: Small.</strong> The same as <code>Large</code>, but with small database.</p>
<p><strong>degrees.py</strong> </p>
<p>Variables:</p>
<ul>
<li>names(dict) | name -&gt; person_ids {maybe people have the same name}</li>
<li>people(dict) | person_id -&gt; name, birth, movies(set)</li>
<li>movies(dict) | movie_id -&gt; title, year, stars(set)</li>
</ul>
<p>Functions:</p>
<ul>
<li><p>load_data: loads data from <code>people.csv</code>, <code>stars.csv</code> and <code>movies.csv</code></p>
</li>
<li><p>main: print the result of one of the shortest path (if it has), or return None when there exists no shortest path.</p>
</li>
<li><p>shortest_path: returns the shortest list of (movies, person_id) pairs</p>
</li>
<li><p>person_id_for_name: returns person_id of the person’s name</p>
</li>
<li><p>neighbor_for_person: returns (movies, person_id) pairs that relates to the given person</p>
</li>
</ul>
<h2 id="Use-BFS-to-find-the-shortest-path"><a href="#Use-BFS-to-find-the-shortest-path" class="headerlink" title="Use BFS to find the shortest path"></a>Use BFS to find the shortest path</h2><p>There are two points that we need to pay attention to. First, we cannot visit the node that we have visited, this is because maybe it could cause circle making this loop never end and it absolutely is not the shortest path when we revisit one node. Second, when we backtrack, the order of the result is the opposite, just reverse it.</p>
<pre><code class="python">def shortest_path(source, target):
    &quot;&quot;&quot;
    Returns the shortest list of (movie_id, person_id) pairs
    that connect the source to the target.

    If no possible path, returns None.
    &quot;&quot;&quot;
    Q = QueueFrontier()
    root = Node(state=source, parent=None, action=None)
    target_leave = Node(state=None, parent=None, action=None)
    Q.add(root)

    _find = False
    path = list()
    unordered_map = dict()
    while (not Q.empty()) and (not _find):
        top_Node = Q.remove()
        if top_Node.state not in unordered_map:
            unordered_map[top_Node.state] = 1
        # find the top node&#39;s neighbors
        neighbors = neighbors_for_person(top_Node.state)
        for neighbor in neighbors:
            movie_id, person_id = neighbor
            if person_id not in unordered_map:
                child = Node(state=person_id, parent=top_Node, action=movie_id)
                if (child.state == target):
                    target_leave = child
                    _find = True
                    break
                Q.add(child)
    if target_leave.state == None:
        return None
    else:
        while target_leave.parent != None:
            # append (&quot;movie_title&quot;, &quot;person_name&quot;)
            path.append((target_leave.action, target_leave.state))
            target_leave = target_leave.parent
    path.reverse()
    return path
</code></pre>
]]></content>
      <tags>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title>Merge Sort and Its Application</title>
    <url>/2023/02/20/Merge-Sort-and-Its-Application/</url>
    <content><![CDATA[<p><strong>Merge Sort Algorithm</strong> is based on the concept of <code>Divide and Conquer</code>, specifically it is traversing a tree, and the process of merging is an action implemented in post order. As we know each child part is in order after merging, we can use this characteristic to address some problems. For example, <a href="https://leetcode.com/problems/reverse-pairs/">Reverse Pairs</a>, when the child is in order, it becomes easy to calculate the number of <code>reverse pair</code> between the left child and the right child.</p>
<span id="more"></span>

<p><img src="https://cdn.jsdelivr.net/gh/LFool/image-hosting@master/20220714/1526101657783570EmOMLr5.svg" alt="5"></p>
<p>The picture above visualizes the process of <code>merge sort algorithm</code>, where we easily recognize the depth of tree is <code>log(N)</code>. And basically we have to sort sequences in merging, the time complexity is <code>O(N)</code>. So this algorithm time complexity is <code>Nlog(N)</code> in total.</p>
<h2 id="Merge-Sort-Algorithm"><a href="#Merge-Sort-Algorithm" class="headerlink" title="Merge Sort Algorithm"></a>Merge Sort Algorithm</h2><p>According the idea from the picture above, we can easily code this program below.</p>
<pre><code class="cpp">class Solution
&#123;
public:
    vector&lt;int&gt; sortArray(vector&lt;int&gt; &amp;nums)
    &#123;
        int len = nums.size();
        if (len &lt; 2) return;
        
        int mid = len &gt;&gt; 1;
        vector&lt;int&gt; leftArray(nums.begin(), nums.begin() + mid);
        vector&lt;int&gt; rightArray(nums.begin() + mid, nums.end());
        
        sort(leftArray);
        sort(rightArray);
        mergeArray(nums, leftArray, rightArray);
        
        return nums;
    &#125;
    
    void mergeArray(vector&lt;int&gt; &amp;nums, vector&lt;int&gt; &amp;leftArray, vector&lt;int&gt; &amp;right)
    &#123;
        int leftSize = leftArray.size(), rightSize = rightArray.size();
        int cur = 0, cur1 = 0, cur2 = 0;
        
        while (cur1 &lt; leftSize &amp;&amp; cur2 &lt; rightSize)
        &#123;
            if (leftArray[cur1] &lt;= rightArray[cur2])
                nums[cur++] = leftArray[cur1++];
               else
                nums[cur++] = rightArray[cur2++];
        &#125;
        
        while (cur1 &lt; leftSize)
            nums[cur++] = leftArray[cur1++];
        while (cur2 &lt; rightSize)
            nums[cur++] = rightArray[cur2++];
    &#125;
&#125;；
</code></pre>
<p>About its application, we always try to find whether a problem can apply the characteristic that the children parts are in order after merging. Here are some problems to apply <code>merge sort algorithm</code>.</p>
<h2 id="Count-of-Smaller-Numbers-After-Self"><a href="#Count-of-Smaller-Numbers-After-Self" class="headerlink" title="Count of Smaller Numbers After Self"></a><a href="https://leetcode.com/problems/count-of-smaller-numbers-after-self/?utm_source=LCUS&utm_medium=ip_redirect&utm_campaign=transfer2china">Count of Smaller Numbers After Self</a></h2><p>Suppose <code>i</code> points the 1st element of the left, <code>j</code> and <code>mid+1</code> points 1st element of the right. When we’re merging, if <code>temp[i]</code> is less <code>temp[j]</code>, we can know that there are <code>j-mid-1</code> elements are less than <code>temp[i]</code>, because the array is monotonically increasing.</p>
<img src="/2023/02/20/Merge-Sort-and-Its-Application/image-20230220153246591.png" alt="image-20230220153246591" style="zoom:50%;">

<pre><code class="cpp">class Solution &#123;
public:
    vector&lt;pair&lt;int, int&gt;&gt; temp;
    vector&lt;int&gt; count;
    vector&lt;int&gt; countSmaller(vector&lt;int&gt;&amp; nums) &#123;
        int n = nums.size();
        vector&lt;pair&lt;int, int&gt;&gt; num_index;
        for (int i = 0; i &lt; n; i++)
            num_index.push_back(pair&lt;int, int&gt;(nums[i], i));
        
        temp = vector&lt;pair&lt;int, int&gt;&gt;(n);
        count = vector&lt;int&gt;(n, 0);

        merge_sort(num_index, 0, n-1);
        return count;
    &#125;
    void merge_sort(vector&lt;pair&lt;int, int&gt;&gt;&amp; num_index, int l, int r)&#123;
        if (l &gt;= r) return;
        int mid = l + (r - l) / 2;
        merge_sort(num_index, l, mid);
        merge_sort(num_index, mid+1, r);
        merge(num_index, l, mid, r);
    &#125;
    void merge(vector&lt;pair&lt;int, int&gt;&gt;&amp; num_index, int l, int mid, int r)&#123;
        int i = l, j = mid + 1;
        int k = l;
        while (i &lt;= mid &amp;&amp; j &lt;= r)&#123;
            if (num_index[i].first &lt;= num_index[j].first)&#123;
                count[num_index[i].second] += j - mid - 1;
                temp[k++] = num_index[i++];
            &#125;
            else temp[k++] = num_index[j++];
        &#125;
        while (i &lt;= mid) &#123;
            count[num_index[i].second] += j - mid - 1; 
            temp[k++] = num_index[i++];
        &#125;
        while (j &lt;= r) temp[k++] = num_index[j++];
        for (i = l; i &lt;= r; i++)
            num_index[i] = temp[i];
    &#125;
&#125;;
</code></pre>
<h2 id="Reverse-Pairs"><a href="#Reverse-Pairs" class="headerlink" title="Reverse Pairs"></a><a href="https://leetcode.com/problems/reverse-pairs/">Reverse Pairs</a></h2><p>This problem is just the same as the last one with a little difference. We assume that we have the ordered left child and right child below. And the next step is merging, but before that, we can calculate the number between the left and the right, <code>betValue</code>. Suppose <code>leftValue</code> as the number  in the left, as well as <code>rightValue</code> in the right. The final result can be calculated recursively.</p>
<p><img src="https://cdn.jsdelivr.net/gh/LFool/image-hosting@master/20220714/1600211657785621E3qWVR6.svg" alt="6"></p>
<p>So, how to get <code>betValue</code>? Just add some codes in the post order space. We can get the first element in the right that is more than <code>nums[i] / 2.0</code>. </p>
<pre><code class="cpp">class Solution
&#123;
public:
    vector&lt;int&gt; tmp;
    int mergeSort(vector&lt;int&gt; &amp;nums, int left, int right)
    &#123;
        if (left &gt;= right)
            return 0;
        int mid = left + ((right - left) &gt;&gt; 1);

        int retLeft = mergeSort(nums, left, mid);
        int retRight = mergeSort(nums, mid + 1, right);

        int cur1 = left, cur2 = mid + 1;
        int ret = 0;

        while (cur1 &lt;= mid)
        &#123;
            while (cur2 &lt;= right &amp;&amp; nums[cur1] / 2.0 &gt; nums[cur2])
                cur2++;
            ret += cur2 - mid - 1;
            cur1++;
        &#125;

        merge(nums, left, mid, right);

        return ret + retLeft + retRight;
    &#125;
    
    merge(vector&lt;int&gt;&amp; numx, int l, int mid, int r) &#123;...&#125;

    int reversePairs(vector&lt;int&gt; &amp;nums)
    &#123;
        int len = nums.size();
        tmp = vector&lt;int&gt;(len, 0);
        return mergeSort(nums, 0, len - 1);
    &#125;
&#125;;
</code></pre>
<h2 id="Count-of-Range-Sum"><a href="#Count-of-Range-Sum" class="headerlink" title="Count of Range Sum"></a><a href="https://leetcode.com/problems/count-of-range-sum/">Count of Range Sum</a></h2><p>It’s the same, but here we need to use <code>Prefix Sum Array</code> and understand why we can use <code>merge sort</code> to solve this problem.</p>
<pre><code class="cpp">class Solution
&#123;
public:
    vector&lt;long&gt; tmp;
    int countRangeSum(vector&lt;int&gt; &amp;nums, int lower, int upper)
    &#123;
        int len = nums.size();
        vector&lt;long&gt; preSum(&#123;0&#125;);
        for (int i = 0; i &lt; len; i++)
            preSum.emplace_back(preSum[i] + nums[i]);
        tmp = vector&lt;long&gt;(preSum.size(), 0);
        return mergeSort(preSum, 0, preSum.size() - 1, lower, upper);
    &#125;

    int mergeSort(vector&lt;long&gt; &amp;nums, int left, int right, int lower, int upper)
    &#123;
        if (left &gt;= right)
            return 0;
        int mid = left + ((right - left) &gt;&gt; 1);

        int retLeft = mergeSort(nums, left, mid, lower, upper);
        int retRight = mergeSort(nums, mid + 1, right, lower, upper);

        int cur1 = mid + 1, cur2 = mid + 1;
        int ret = 0;
        for (int i = left; i &lt;= mid; i++)
        &#123;
            while (cur1 &lt;= right &amp;&amp; nums[cur1] - nums[i] &lt; lower)
                cur1++;
            while (cur2 &lt;= right &amp;&amp; nums[cur2] - nums[i] &lt;= upper)
                cur2++;
            ret += cur2 - cur1;
        &#125;

        merge(nums, left, mid, right);
        return ret + retLeft + retRight;
    &#125;
    merge(vector&lt;int&gt;&amp; numx, int l, int mid, int r) &#123;...&#125;
&#125;;
</code></pre>
]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Merge Sort</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Policy</title>
    <url>/2023/08/22/Diffusion-Policy/</url>
    <content><![CDATA[<p>Diffusion model has already been used in computer vision for such a long time. However, in RSS 2023, it’s also used in Robotics, achieving a promising performance.</p>
<span id="more"></span>

<h1 id="Diffusion-Policy"><a href="#Diffusion-Policy" class="headerlink" title="Diffusion Policy"></a>Diffusion Policy</h1><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>:arrow_forward: <strong>Original Data in HDF5 File</strong></p>
<p>The official provides dataset in <code>hdf5</code> format. The <code>hdf5</code> file saves many stuffs of demonstrations, such as actions, dones(whether the episode is done), rewards, states(vectors describing the robot states), and obs. However, for training the policy, here just uses a part of this dataset.</p>
<pre><code class="yaml">Group: /data/demo_0
  Dataset: /data/demo_0/actions    shape: (127, 7)
  Dataset: /data/demo_0/dones    shape: (127,)
  Dataset: /data/demo_0/rewards    shape: (127,)
  Dataset: /data/demo_0/states    shape: (127, 45)
  Group: /data/demo_0/next_obs
    Dataset: /data/demo_0/next_obs/agentview_image    shape: (127, 84, 84, 3)
    Dataset: /data/demo_0/next_obs/object    shape: (127, 14)
    Dataset: /data/demo_0/next_obs/robot0_eef_pos    shape: (127, 3)
    Dataset: /data/demo_0/next_obs/robot0_eef_quat    shape: (127, 4)
    Dataset: /data/demo_0/next_obs/robot0_eef_vel_ang    shape: (127, 3)
    Dataset: /data/demo_0/next_obs/robot0_eef_vel_lin    shape: (127, 3)
    Dataset: /data/demo_0/next_obs/robot0_eye_in_hand_image    shape: (127, 84, 84, 3)
    Dataset: /data/demo_0/next_obs/robot0_gripper_qpos    shape: (127, 2)
    Dataset: /data/demo_0/next_obs/robot0_gripper_qvel    shape: (127, 2)
    Dataset: /data/demo_0/next_obs/robot0_joint_pos    shape: (127, 7)
    Dataset: /data/demo_0/next_obs/robot0_joint_pos_cos    shape: (127, 7)
    Dataset: /data/demo_0/next_obs/robot0_joint_pos_sin    shape: (127, 7)
    Dataset: /data/demo_0/next_obs/robot0_joint_vel    shape: (127, 7)
  Group: /data/demo_0/obs
    Dataset: /data/demo_0/obs/agentview_image    shape: (127, 84, 84, 3)
    Dataset: /data/demo_0/obs/object    shape: (127, 14)
    Dataset: /data/demo_0/obs/robot0_eef_pos    shape: (127, 3)
    Dataset: /data/demo_0/obs/robot0_eef_quat    shape: (127, 4)
    Dataset: /data/demo_0/obs/robot0_eef_vel_ang    shape: (127, 3)
    Dataset: /data/demo_0/obs/robot0_eef_vel_lin    shape: (127, 3)
    Dataset: /data/demo_0/obs/robot0_eye_in_hand_image    shape: (127, 84, 84, 3)
    Dataset: /data/demo_0/obs/robot0_gripper_qpos    shape: (127, 2)
    Dataset: /data/demo_0/obs/robot0_gripper_qvel    shape: (127, 2)
    Dataset: /data/demo_0/obs/robot0_joint_pos    shape: (127, 7)
    Dataset: /data/demo_0/obs/robot0_joint_pos_cos    shape: (127, 7)
    Dataset: /data/demo_0/obs/robot0_joint_pos_sin    shape: (127, 7)
    Dataset: /data/demo_0/obs/robot0_joint_vel    shape: (127, 7)
</code></pre>
<p>:arrow_forward: <strong>observation</strong> </p>
<p>Here observation includes an agent view image, a robot image from its hand, end effector’s positions and quaternion, and robot gripper positions. </p>
<pre><code class="tex">agentview_image:
  shape: [3, 84, 84]
  type: rgb
robot0_eye_in_hand_image:
  shape: [3, 84, 84]
  type: rgb
robot0_eef_pos:
  shape: [3]
  # type default: low_dim
robot0_eef_quat:
  shape: [4]
robot0_gripper_qpos:
  shape: [2]
</code></pre>
<p>:arrow_forward: <strong>action</strong></p>
<p>The first three dimension of action is to describe end effector’s position change, and the subsequent three dimension is to illustrate rotation change, and the last dimension is to record gripper’s status.</p>
<pre><code>desired translation of EEF(3), desired delta rotation from current EEF(3), and opening and closing of the gripper fingers:
    shape: [7]
</code></pre>
<h2 id="HyperParameters"><a href="#HyperParameters" class="headerlink" title="HyperParameters"></a>HyperParameters</h2><p>DDMP algorithm hyperparameters of policy, it can affect the denoising performance.</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>beta_start</td>
<td>the starting beta value of inference</td>
<td>0.0001</td>
</tr>
<tr>
<td>beta_end</td>
<td>the final beta value</td>
<td>0.02</td>
</tr>
<tr>
<td>beta_schedule</td>
<td>the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model</td>
<td>squaredcos_cap_v2</td>
</tr>
</tbody></table>
<p>Task configuration of policy.</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>horizon</td>
<td>the step number of predicted action</td>
<td>10</td>
</tr>
<tr>
<td>n_action_steps</td>
<td>the step number of executing action</td>
<td>8</td>
</tr>
<tr>
<td>n_obs_steps</td>
<td>the step number of obs that the model prediction depends</td>
<td>2</td>
</tr>
</tbody></table>
<p>Image processing of policy</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>crop_shape</td>
<td>the target image dimension after cropping</td>
<td>10</td>
</tr>
</tbody></table>
<p>Hyperparameters of model(transformer) that the policy uses.</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>n_layer</td>
<td>the layer of decoder&#x2F;encoder</td>
<td>8</td>
</tr>
<tr>
<td>n_head</td>
<td>head number of multi-head attention</td>
<td>4</td>
</tr>
<tr>
<td>n_emb</td>
<td>embedding dimension</td>
<td>256</td>
</tr>
<tr>
<td>p_drop_emb</td>
<td>drop prob of nn.Dropout before encoder&#x2F;decoder</td>
<td>0.0</td>
</tr>
<tr>
<td>p_drop_attn</td>
<td>drop prob of nn.Dropout in transformer layer</td>
<td>0.3</td>
</tr>
</tbody></table>
<p>EMA parameters.</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>inv_gamma</td>
<td>inverse multiplicative factor of EMA warmup</td>
<td>1.0</td>
</tr>
<tr>
<td>power</td>
<td>exponential factor of EMA warup</td>
<td>0.75</td>
</tr>
<tr>
<td>min_value</td>
<td>the minimum EMA decay rate</td>
<td>0.0</td>
</tr>
<tr>
<td>max_value</td>
<td>the maximum EMA decay rate</td>
<td>0.9999</td>
</tr>
</tbody></table>
<p>dataloader：</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>batch_size</td>
<td>batch size</td>
<td>64</td>
</tr>
<tr>
<td>num_workers</td>
<td>number of processes when loading data</td>
<td>8</td>
</tr>
</tbody></table>
<p>optimizer:</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>transformer_weight_decay</td>
<td>transformer weight decay</td>
<td>1.0e-3</td>
</tr>
<tr>
<td>obs_encoder_weight_decay</td>
<td>obs encoder weight decay</td>
<td>1.0e-6</td>
</tr>
<tr>
<td>learning_rate</td>
<td>learning rate</td>
<td>1.0e-4</td>
</tr>
<tr>
<td>betas</td>
<td>decay rate of first-order moment and second-order moment</td>
<td>[0.9, 0.95]</td>
</tr>
</tbody></table>
<pre><code class="yaml">policy: # policy configuration
    _target_: DiffusionTransformerHybridImagePolicy # policy type
    
    shape_meta: # observations and actions specification
        obs:
            agentview_image:
                shape: [3, 84, 84]
                type: rgb
            robot0_eye_in_hand_image:
                shape: [3, 84, 84]
                type: rgb
            robot0_eef_pos:
                shape: [3]
                # type default: low_dim
            robot0_eef_quat:
                shape: [4]
            robot0_gripper_qpos:
                shape: [2]
        action: 
            shape: [7]
    
    noise_scheduler: # DDPM algorithm&#39;s hyperparameters
        _target: DDPMScheduler	# algorithm type
        num_train_timesteps: 100
        beta_start: 0.0001
        beta_end: 0.02
        beta_schedule: squaredcos_cap_v2
        variance_type: fixed_small # Yilun&#39;s paper uses fixed_small_log instead, but easy to cause Nan
        clip_sample: True # required when predict_epsilon=False
        prediction_type: epsilon # or sample
    # task cfg
    horizon: 10 # dataset sequence length
    n_action_steps: 8	# number of steps of action will be executed
    n_obs_steps: 2 # the latest steps of observations data as input
    num_inference_steps: 100
    # image cfg
    crop_shape: [76, 76]	# images will be cropped into [76, 76]
    obs_encoder_group_norm: False,
    # arch
    n_layer: 8	# transformer decoder/encoder layer number
    n_cond_layers: 0  # &gt;0: use transformer encoder for cond, otherwise use MLP
    n_head: 4	# head number
    n_emb: 256	# embedding dim (input dim --(emb)--&gt; n_emb)
    p_drop_emb: 0.0	# dropout prob (before encoder&amp;decoder)
    p_drop_attn: 0.3	# encoder_layer dropout prob
    causal_attn: True	# mask or not
    time_as_cond: True # if false, use BERT like encoder only arch, time as input
    obs_as_cond: True

# if ema is true
ema:
    _target_: diffusion_policy.model.diffusion.ema_model.EMAModel
    update_after_step: 0
    inv_gamma: 1.0
    power: 0.75
    min_value: 0.0
    max_value: 0.9999
dataloader:
    batch_size: 64
    num_workers: 8
    shuffle: True
    pin_memory: True
    persistent_workers: False

val_dataloader:
    batch_size: 64
    num_workers: 8
    shuffle: False
    pin_memory: True
    persistent_workers: False

optimizer:
    transformer_weight_decay: 1.0e-3
    obs_encoder_weight_decay: 1.0e-6
    learning_rate: 1.0e-4
    betas: [0.9, 0.95]

training:
    device: &quot;cuda:0&quot;
    seed: 42
    debug: False
    resume: True
    # optimization
    lr_scheduler: cosine
    # Transformer needs LR warmup
    lr_warmup_steps: 10
    num_epochs: 100
    gradient_accumulate_every: 1
    # EMA destroys performance when used with BatchNorm
    # replace BatchNorm with GroupNorm.
    use_ema: True
    # training loop control
    # in epochs
    rollout_every: 10
    checkpoint_every: 10
    val_every: 1
    sample_every: 5
    # steps per epoch
    max_train_steps: null
    max_val_steps: null
    # misc
    tqdm_interval_sec: 1.0
</code></pre>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><pre><code class="yaml">obs:
    agentview_image:
        shape: [bs, T, 3, 84, 84]
        type: rgb
    robot0_eye_in_hand_image:
        shape: [bs, T, 3, 84, 84]
        type: rgb
    robot0_eef_pos:
        shape: [bs, T, 3]
    robot0_eef_quat:
        shape: [bs, T, 4]
    robot0_gripper_qpos:
        shape: [bs, T, 2]
action: 
    shape: [bs, T, 7]
timesteps:
    shape: [1]
</code></pre>
<p><img src="/2023/08/22/Diffusion-Policy/image_1.png" alt="Overall Structure"></p>
<p>The picture above describes the overall structure of the training process. We need 3 types of inputs whose definition is introduced in the above code box. Before passing to transformer block, we do preprocessing, like adding noise to action, generating timesteps randomly, and using <code>obs_encoder</code> to extract features from observations.</p>
<pre><code class="python">&quot;&quot;&quot; 1. normalize obs &amp; action -&gt; nobs &amp; naction &quot;&quot;&quot;
nobs = self.normalizer.normalize(batch[&#39;obs&#39;])
nactions = self.normalizer[&#39;action&#39;].normalize(batch[&#39;action&#39;])
trajectory = nactions

&quot;&quot;&quot; 2. take the subsequence of the first To in nobs and do feature extraction with obs_encoder &quot;&quot;&quot;
# reshape B, T, ... to B*T
this_nobs = dict_apply(nobs, lambda x: x[:,:To,...].reshape(-1,*x.shape[2:]))
nobs_features = self.obs_encoder(this_nobs)
# reshape back to B, T, Do
cond = nobs_features.reshape(batch_size, To, -1)

&quot;&quot;&quot; 3. add noise to actions &quot;&quot;&quot;
noise = torch.randn(trajectory.shape, device=trajectory.device)
# Add noise to the clean images according to the noise magnitude at each timestep
# (this is the forward diffusion process)
noisy_trajectory = self.noise_scheduler.add_noise(
    trajectory, noise, timesteps)

&quot;&quot;&quot; 4. generate timesteps randomly &quot;&quot;&quot;
bsz = trajectory.shape[0]
# Sample a random timestep for each image
timesteps = torch.randint(
    0, self.noise_scheduler.config.num_train_timesteps, 
    (bsz,), device=trajectory.device
).long()
</code></pre>
<p>Step 2 can be explained in <a href="#visual encoder">Visual Encoder</a>.</p>
<p>Step 3 can be explained in <a href="#add noise">Add Noise</a>.</p>
<h3 id="Visual-Encoder-how-obs-encoder-extract-features-from-obs"><a href="#Visual-Encoder-how-obs-encoder-extract-features-from-obs" class="headerlink" title="Visual Encoder (how obs_encoder extract features from obs)"></a><span id="visual encoder">Visual Encoder</span> (how obs_encoder extract features from obs)</h3><p>In order to get <code>cond</code>, here has a <span id="obs_encoder">obs_encoder</span> to get features from observations, including images and robot states staff. The encoder is from <code>robomimic</code> package, which is a <code>ObservationGroupEncoder</code> class below. This class is designed to process multiple observations, so one of its arguments is <code>observation_group_shapes</code>, which describes shapes of every observation. And here lists one example of this. </p>
<pre><code class="python">&quot;&quot;&quot;
example of observation_group_shapes:

OrderedDict([(&#39;obs&#39;, 
        OrderedDict([
            (&#39;agentview_image&#39;, [3, 84, 84]), 
            (&#39;robot0_eye_in_hand_image&#39;, [3, 84, 84]), 
            (&#39;robot0_eef_pos&#39;, [3]), 
            (&#39;robot0_eef_quat&#39;, [4]), 
            (&#39;robot0_gripper_qpos&#39;, [2])
            ]))])
&quot;&quot;&quot;
class ObservationsGroupEncoder(Module):
    &quot;&quot;&quot;
    This class allows networks to encode multiple observation dictionaries into a single flat, concatenated vector representation.
    It does this by assigning each observation dictionary (observation group) an @ObservationEncoder Object.
    
    This class takes a dictionary of dictionaries, @observation_group_shapes.
    Each key corresponds to an observation group (e.g. &#39;obs&#39;, &#39;subgoal&#39;, &#39;goal&#39;)
    and each OrderedDict should be a map between modalities and expected input shape (e.g. &#123;&#39;image&#39;: (3, 120, 160)&#125;)
    &quot;&quot;&quot;
    def __init__(
        self,
        observation_group_shapes,
        feature_activation=nn.ReLU,
        encoder_kwargs=None,):
        &quot;&quot;&quot;
        Args:
            observation_group_shapes (OrderedDict): a dictionary of dictionaries.
                Each key in this dictionary should specify an observation group,
                and the value should be an OrderedDict that maps modalities to expected shapes.
            
            feature_activation: non-linearity to apply after each obs net - defaults to ReLU.
            
            encoder_kwargs (dict or None): If None, results in default encoder_kwargs being applied.
                Otherwise, should be nested dictionary containing relevant per-modality information for encoder networks.
                
                should be of form:
                
                obs_modality1: dict
                    feature_dimension: int
                    core_class: str
                    core_kwargs: dict
                        ...
                        ...
                    obs_randomizer_class: str
                    obs_randomizer_kwargs: dict
                        ...
                        ...
                obs_modality2: dict
                    ...
            &quot;&quot;&quot;
            self.observation_group_shapes = observation_group_shapes
            # create an observation encoder per observation group
</code></pre>
<p>Because it can process multiple observations, which means that it has multiple networks for different inputs. Here we have 5 observations, so we have 5 networks. Take agentview_image as an example, its network is established with backbone (resnet18) and pool layers. For such robot0_eef_pos low dimension observation, its network is None. Noticeably, every network here will turn the observation into a 2-dim vector, i.e. [batch_size, output_shape]. Finally, we concatenate all outputs in <code>dim=1</code>, so here is [batch_size, 64+64+3+4+2], i.e. [batch_size, 137].</p>
<pre><code class="txt">ObservationEncoder(
    Key(
        name=agentview_image
        shape=[3, 84, 84]
        modality=rgb
        randomizer=CropRandomizer(input_shape=[3, 84, 84], crop_size=[76, 76], num_crops=1)
        net=VisualCore(
          input_shape=[3, 76, 76]
          output_shape=[64]
          backbone_net=ResNet18Conv(input_channel=3, input_coord_conv=False)
          pool_net=SpatialSoftmax(num_kp=32, temperature=1.0, noise=0.0)
        )
        sharing_from=None
    )
    Key(
        name=robot0_eye_in_hand_image
        shape=[3, 84, 84]
        modality=rgb
        randomizer=CropRandomizer(input_shape=[3, 84, 84], crop_size=[76, 76], num_crops=1)
        net=VisualCore(
          input_shape=[3, 76, 76]
          output_shape=[64]
          backbone_net=ResNet18Conv(input_channel=3, input_coord_conv=False)
          pool_net=SpatialSoftmax(num_kp=32, temperature=1.0, noise=0.0)
        )
        sharing_from=None
    )
    Key(
        name=robot0_eef_pos
        shape=[3]
        modality=low_dim
        randomizer=None
        net=None
        sharing_from=None
    )
    Key(
        name=robot0_eef_quat
        shape=[4]
        modality=low_dim
        randomizer=None
        net=None
        sharing_from=None
    )
    Key(
        name=robot0_gripper_qpos
        shape=[2]
        modality=low_dim
        randomizer=None
        net=None
        sharing_from=None
    )
    output_shape=[137]
)
</code></pre>
<p>The Visual Encoder is not pre-trained model, it will be train with transformer at the same time.</p>
<h3 id="Add-Noise"><a href="#Add-Noise" class="headerlink" title="Add Noise"></a><span id="add noise">Add Noise</span></h3><p>The adding noise process can be computed through the formula below.</p>
<p>$x_t&#x3D;\sqrt{\overline{\alpha_t}}x_0+\sqrt{1-\overline{\alpha_t}}\epsilon$</p>
<p>$x_0$ is the original sample, $\epsilon$ is the noise. $\beta_t $ is the forward process variances of timestep $t$. And it has $\alpha_t&#x3D;1-\beta_t$. So the add_noise function can be displayed below.</p>
<pre><code class="python">def add_noise(original_samples, noise, timesteps):
    sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5
    sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5
    
    noise_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise
    return noise_samples
</code></pre>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p><img src="/2023/08/22/Diffusion-Policy/image_1.png" alt="Overall Structure"></p>
<p>Transformer based on diffusion policy is actually one noise predictor. Take in noised data with some conditions, it can predict the noise in the data, and then restore its original data. </p>
<p>The transformer can be seen in the blue dash box in the picture above. After data preprocessing, we have noised sample&#x2F;actions, obs_features&#x2F;cond, and timesteps generated randomly as inputs.</p>
<p>Inputs:</p>
<ul>
<li><p><code>sample</code> is a sequence of noised actions.</p>
</li>
<li><p><code>cond</code> denotes the observation feature. </p>
</li>
<li><p><code>timesteps</code> is the number of diffusion steps.</p>
</li>
</ul>
<p><code>Encoder</code> is designed to  encode observation features and timesteps. <code>n_cond_layers</code> is a hyperparameter that can be set in configuration files, and if it’s &gt; 0, the transformer encoder will replace MLP encoder. </p>
<p><code>Decoder</code> takes in noised actions and encoded information, then predicts a noise with the same shape of X&#x2F;sample as output.</p>
<p>Both transformer encoder and decoder are using torch.nn module, and the transformer forward computation is shown in the code box below.</p>
<blockquote>
<p>Its structure is based on minGPT, which is decoder-only. Here it means that the input <code>sample</code> (noised actions) will only being processed by the transformer decoder, which means that it does not need to learning information from <code>sample</code> by encoder, conversely it just needs to do the noise prediction task by decoder. Details are below, encoder is to process <code>cond</code> and <code>timestep</code> only, and decoder is to process <code>sample</code> only.</p>
</blockquote>
<pre><code class="python">&quot;&quot;&quot;
input arguments:
    sample: A sequence of noised actions.
    cond: Observation features.
    timesteps: diffusion step.
&quot;&quot;&quot;
# 1. inputs embedding
time_emb = self.time_emb(timesteps)
input_emb = self.input_emb(sample)
cond_obs_emb = self.cond_obs_emb(cond)

# 2. prepare transformer encoder inputs, including concatenating and adding position embeddings.
cond_embeddings = torch.cat([time_emb, cond_obs_emb], dim=1)
tc = cond_embeddings.shape[1]
position_embeddings = self.cond_pos_emb[
    :, :tc, :
]  # each position maps to a (learnable) vector
x = self.drop(cond_embeddings + position_embeddings)

# 3. process encoder inputs by encoder
x = self.encoder(x)
memory = x

# 4. prepare decoder inputs - embedded sample + position embedding
token_embeddings = input_emb
t = token_embeddings.shape[1]
position_embeddings = self.pos_emb[
    :, :t, :
]  # each position maps to a (learnable) vector
x = self.drop(token_embeddings + position_embeddings)

# 5. using preprocessed sample and condition information to predict noise by decoder
x = self.decoder(
    tgt=x,
    memory=memory,
    tgt_mask=self.mask,
    memory_mask=self.memory_mask
)
</code></pre>
<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>The training loss is below, the goal is to train a policy $\epsilon_{\theta}$ to predict noise accurately:</p>
<p>$Loss &#x3D; MSE(\epsilon^k, \epsilon_{\theta}(O_t, A_t^0+\epsilon^k,k))$</p>
<p>where $\epsilon^k$ is a random noise with appropriate variance for iteration k. $O_t$ is the observation features.</p>
<p>After predicting noise by transformer, we got <code>pred_noise</code>. Then we use it to calculate training loss with the formula above. </p>
<pre><code class="python"># 1. Predict the noise residual
pred_noise = self.model(noisy_trajectory, timesteps, cond)

# 2. calculate training loss
loss = F.mse_loss(pred, target, reduction=&#39;none&#39;)
loss = loss * loss_mask.type(loss.dtype)
loss = reduce(loss, &#39;b ... -&gt; b (...)&#39;, &#39;mean&#39;)
loss = loss.mean()y
</code></pre>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>After we got a trained policy $\epsilon_{\theta}$. We use the following formula to inference.</p>
<p>$A_t^{k-1}&#x3D;\alpha(A_t^k-\gamma\epsilon_{\theta}(O_t,A_t^k,k)+N(0,\sigma^2I))$</p>
<p>When doing inference&#x2F;testing&#x2F;rollout, it will use predict_action function of policy in <code>env_runner</code>. The difference between inference and training is that, it would not do backward to update parameters, secondly instead of just getting noise to compute loss, it uses <code>noise_scheduler.step</code> to acquire the original trajectory.</p>
<p>First, we introduce how <code>env_runner</code> works. We can simply decompose the simulation process into 2 steps, i.e. running policy to get predicted actions and stepping environment with predicted actions. </p>
<blockquote>
<p>Further, <code>env_runner</code> uses multiprocessing to achieve multiple environment to execute parallelly. And these simulation environments have 2 types - train and test. Here train type means the initial state is from original dataset, and test type means initial state is set randomly with a different seed.</p>
</blockquote>
<pre><code class="python">while not done:
    # run policy
    with torch.no_grad():
        action_dict = policy.predict_action(obs_dict)

    # step env
    env_action = action
    if self.abs_action:
        env_action = self.undo_transform_action(action)

    obs, reward, done, info = env.step(env_action)
    done = np.all(done)
    past_action = action
</code></pre>
<p>Below is the details of how to implement predicting actions. </p>
<pre><code class="python"># 1. randomly generate a trajectory.
trajectory = torch.randn(
    size=condition_data.shape, 
    dtype=condition_data.dtype,
    device=condition_data.device,
    generator=generator)

# 2. set step values
scheduler.set_timesteps(self.num_inference_steps)

# 3. use scheduler.step to get original trajectory in a loop
for t in scheduler.timesteps:
    # predict noise
    model_output = model(trajectory, t, cond)

    # compute previous image: x_t -&gt; x_t-1
    trajectory = scheduler.step(
        model_output, t, trajectory, 
        generator=generator,
        **kwargs
    ).prev_sample    
# finally we get trajectory_0
</code></pre>
<p>Here is the algorithm of <code>noise_scheduler.step</code>. (x_t -&gt; x_t-1)</p>
<pre><code class="python"># 1. compute alphas, betas
# 2. compute predicted original sample from predicted noise also called
# 3. Clip &quot;predicted x_0&quot;
# 4. Compute coefficients for pred_original_sample x_0 and current sample x_t
# 5. Compute predicted previous sample µ_t
pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample
# 6. Add noise
pred_prev_sample = pred_prev_sample + variance
</code></pre>
<h2 id="Issues-on-Different-Ways-of-Passing-Observations"><a href="#Issues-on-Different-Ways-of-Passing-Observations" class="headerlink" title="Issues on Different Ways of Passing Observations"></a>Issues on Different Ways of Passing Observations</h2><p>We know that observation features is a input of transformer, but actually it has 2 ways to pass observation features.</p>
<p><strong>Regard Observations as Condition</strong></p>
<p>This way means that, the observation features will be processed by transformer encoder first. Then pass it as conditions to decoder for noise prediction.</p>
<pre><code class="python">cond = None
trajectory = nactions

# reshape B, T, ... to B*T
this_nobs = dict_apply(nobs, lambda x: x[:,:To,...].reshape(-1,*x.shape[2:]))
nobs_features = self.obs_encoder(this_nobs)
# reshape back to B, To, Do
cond = nobs_features.reshape(batch_size, To, -1)
</code></pre>
<p>Another way means that it would not pass the observation features to transformer encoder. Instead, it will regard the observation features as a part of sample. Here, we know that <code>cond</code> is set None, which proves that observation features are not passed to transformer encoder.</p>
<pre><code class="python">cond = None
trajectory = nactions

# reshape B, T, ... to B*T
this_nobs = dict_apply(nobs, lambda x: x.reshape(-1, *x.shape[2:]))
nobs_features = self.obs_encoder(this_nobs)
# reshape back to B, T, Do
nobs_features = nobs_features.reshape(batch_size, horizon, -1)
trajectory = torch.cat([nactions, nobs_features], dim=-1).detach()
</code></pre>
<blockquote>
<p>Here is my point. Looking back ACT model structure, we regard it as a conditional VAE because it not only uses latent code from ENCODER to generate, it also uses observations (camera images, joint positions&#x2F;torques). We regard these observations as conditions, because they are all processed by transformer encoder, and then are passed to transformer decoder to impact its decoding process. So the second passing observation way here, may cannot regard observation as conditions. </p>
<p>However, the point of the second way, I guess, is to make the model not just to predict noise of actions, but also predict noise of observations, i.e. restoring actions and observations at the same time. It’s like only such observations, we do such specific actions. To some extent, the actions are connected to observations. </p>
</blockquote>
<h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><ol>
<li>Specify the values of alpha, gamma in the denoising process.</li>
</ol>
<blockquote>
<p>gamma is the learning rate. alpha is a weight to denote the importance of noise.</p>
</blockquote>
<ol start="2">
<li>Specify the value of the variance for iteration k (with explanation).</li>
</ol>
<pre><code class="python"># first sample a variance noise
variance_noise = torch.randn(model_output.shape, dtype=model_output.dtype, generator=generator)
# use _get_variance to get variance of timestep k
variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * self.betas[t]
variance = torch.clamp(variance, min=1e-20)
# finally do 
variance = (self._get_variance(t, predicted_variance=predicted_variance) ** 0.5) * variance_noise
</code></pre>
<ol start="3">
<li>The Visual Encoder is missing. Add it before the diffusion transformer.</li>
</ol>
<blockquote>
<p>done, see <a href="#obs_encoder">obs_encoder</a></p>
</blockquote>
<ol start="4">
<li>The diffusion transformer adopts the architecture from the minGPT (check it out), which is a decoder-only variant of the Transformer. Modify the content accordingly.</li>
</ol>
<blockquote>
<p>See <a href="#Forward Details">Forward Details</a></p>
</blockquote>
<ol start="5">
<li>noised action &#x3D; noised action execution sequence?</li>
</ol>
<blockquote>
<p>No, the shape of noised action is (B, T, Da), but the shape of noised action execution sequence is (B, n_action_steps, Da). Noticeably, T &gt;&#x3D; n_action_steps</p>
</blockquote>
<ol start="6">
<li>What is the format of the observation feature?</li>
</ol>
<blockquote>
<p>First, we take the subsequence of the first To observations and reshape it, and the make it processed by obs_encoder to get nobs_features, finally we do <code>nobs_features.reshape(B, To, -1)</code> to reshape obs features.</p>
</blockquote>
<ol start="7">
<li>What is bs in [bs, horizon, action_dim]? Why the dimension has three situations?</li>
</ol>
<blockquote>
<p>bs means batch_size. Because it needs to consider whether regarding observations as a condition. If no, the shape of the output is like (B, T, Da+Do), which uses impainting method to replace action with obs features. If yes, then consider whether predicting action steps only, output shape is (B, n_action_steps, Da) when predicting action steps only, else (B, T, Da).</p>
</blockquote>
]]></content>
      <tags>
        <tag>Imitation Learning</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
</search>
